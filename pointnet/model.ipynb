{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fbe6c3ff-3fd1-4daf-ab11-04b3bc373862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Optional\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bbcb1ee-2541-4f7c-8680-27eabe8629d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/root/share/pointnet.pytorch/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2b4a799d-433d-44a0-92ac-a30cf8df9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import dataset_split\n",
    "from utils.config_utils import flatten_dict\n",
    "from utils.config_utils import register_config\n",
    "from utils.config_utils import configure_optimizers_from_cfg\n",
    "from utils.config_utils import get_loggers\n",
    "from utils.config_utils import get_callbacks\n",
    "from utils.custom_math import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0ef6695-6340-44e9-a1aa-6bdbb4452dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLightningModule(pl.LightningModule):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.loss_function = F.nll_loss()\n",
    "        # self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, inputs, target):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self._optimizer, self._schedulers = configure_optimizers_from_cfg(self.cfg, self)\n",
    "        return self._optimizer, self._schedulers\n",
    "\n",
    "    def _forward(self, images, labels, mode: str):\n",
    "\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        current_loss = 0.0\n",
    "        current_corrects = 0\n",
    "\n",
    "        # get predictions\n",
    "        #outputs = self(images)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # get loss(Loss 계산)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        corrects = torch.sum(preds == labels.data)\n",
    "        acc = corrects / len(outputs)\n",
    "\n",
    "        return {\n",
    "            f\"{mode}_loss\": loss,\n",
    "            f\"{mode}_acc\": acc,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logs = self._forward(images, labels, mode=\"train\")\n",
    "        self.log_dict(logs)\n",
    "        logs[\"loss\"] = logs[\"train_loss\"]\n",
    "        cul_lr = self._optimizer.param_groups[0][\"lr\"] if self._schedulers is None else self._schedulers[0].get_last_lr()[0]\n",
    "        wandb.log({\"learning_rate\": cul_lr})\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logs = self._forward(images, labels, mode=\"val\")\n",
    "        self.log_dict(logs)\n",
    "        logs[\"loss\"] = logs[\"val_loss\"]\n",
    "        return logs\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        logs = self._forward(images, labels, mode=\"test\")\n",
    "        self.log_dict(logs)\n",
    "        logs[\"loss\"] = logs[\"test_loss\"]\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fc08bc7-57b7-4ec9-b695-31d26a708413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetCls(BaseLightningModule):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        BaseLightningModule.__init__(self, cfg=cfg)\n",
    "        \n",
    "        self.feat = PointNetfeat(**cfg)\n",
    "        self.fc1 = nn.Linear(cfg.model.cls.layer1.in_feature, cfg.model.cls.layer1.out_feature)\n",
    "        self.fc2 = nn.Linear(cfg.model.cls.layer2.in_feature, cfg.model.cls.layer2.out_feature)\n",
    "        self.fc3 = nn.Linear(cfg.model.cls.layer3.in_feature, cfg.model.cls.layer3.k)\n",
    "        self.dropout = nn.Dropout(cfg.model.cls.dropout_prob)\n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.cls.layer1.out_feature)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.cls.layer2.out_feature)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1), trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e9c26e6-d7de-4fbc-b158-ba760fda1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetDenseCls(BaseLightningModule):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        BaseLightningModule.__init__(self, cfg=cfg)\n",
    "        \n",
    "        self.k = cfg.model.densecls.k\n",
    "        self.feat = PointNetfeat(**cfg)\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer1.conv1d_in_channel,\n",
    "            cfg.model.densecls.layer1.conv1d_out_channel,\n",
    "            cfg.model.densecls.layer1.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer2.conv1d_in_channel,\n",
    "            cfg.model.densecls.layer2.conv1d_out_channel,\n",
    "            cfg.model.densecls.layer2.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer3.conv1d_in_channel,\n",
    "            cfg.model.densecls.layer3.conv1d_out_channel,\n",
    "            cfg.model.densecls.layer3.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv4 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer4.conv1d_in_channel,\n",
    "            self.k,\n",
    "            cfg.model.densecls.layer3.conv1d_kernel_size\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.densecls.layer1.conv1d_out_channel)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.densecls.layer2.conv1d_out_channel)\n",
    "        self.bn3 = nn.BatchNorm1d(cfg.model.densecls.layer3.conv1d_out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
    "        x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b09a6d9e-6709-4a33-96a2-89ae15a0a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            cfg.model.feat.layer1.conv1d_in_channel,\n",
    "            cfg.model.feat.layer1.conv1d_out_channel,\n",
    "            cfg.model.feat.layer1.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.feat.layer2.conv1d_in_channel,\n",
    "            cfg.model.feat.layer2.conv1d_out_channel,\n",
    "            cfg.model.feat.layer2.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.feat.layer3.conv1d_in_channel,\n",
    "            cfg.model.feat.layer3.conv1d_out_channel,\n",
    "            cfg.model.feat.layer3.conv1d_kernel_size\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.feat.layer1.conv1d_out_channel)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.feat.layer2.conv1d_out_channel)\n",
    "        self.bn3 = nn.BatchNorm1d(cfg.model.feat.layer3.conv1d_out_channel)\n",
    "        self.global_feat = cfg.model.global_feat\n",
    "        self.feature_transform = cfg.model.feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(**cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7afa8dbd-b64d-47cf-9954-c7de036bb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STN3d(nn.Module):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        conv1 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer1.conv1d_in_channel,\n",
    "            cfg.model.stn.layer1.conv1d_out_channel,\n",
    "            cfg.model.stn.layer1.conv1d_kernel_size\n",
    "        )\n",
    "\n",
    "        conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer2.conv1d_in_channel,\n",
    "            cfg.model.stn.layer2.conv1d_out_channel,\n",
    "            cfg.model.stn.layer2.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer3.conv1d_in_channel,\n",
    "            cfg.model.stn.layer3.conv1d_out_channel,\n",
    "            cfg.model.stn.layer3.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        fc1 = nn.Linear(cfg.model.stn.layer4.fc1_in_features, cfg.model.stn.layer4.fc1_out_features)\n",
    "        fc2 = nn.Linear(cfg.model.stn.layer5.fc2_in_features, cfg.model.stn.layer5.fc2_out_features)\n",
    "        fc3 = nn.Linear(cfg.model.stn.layer6.fc3_in_features, cfg.model.stn.layer6.fc3_out_features)\n",
    "    \n",
    "        bn1 = nn.BatchNorm1d(cfg.model.stn.layer1.conv1d_out_channel)\n",
    "        bn2 = nn.BatchNorm1d(cfg.model.stn.layer2.conv1d_out_channel)\n",
    "        bn3 = nn.BatchNorm1d(cfg.model.stn.layer3.conv1d_out_channel)\n",
    "        bn4 = nn.BatchNorm1d(cfg.model.stn.layer4.fc1_out_features)\n",
    "        bn5 = nn.BatchNorm1d(cfg.model.stn.layer5.fc3_out_features)\n",
    "        \n",
    "        relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57c320e8-49cb-4da5-aaec-f60535b4647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STNkd(nn.Module):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.k = model.stn.k\n",
    "        \n",
    "        conv1 = torch.nn.Conv1d(\n",
    "            self.k,\n",
    "            cfg.model.stn.layer1.conv1d_out_channel,\n",
    "            cfg.model.stn.layer1.conv1d_kernel_size\n",
    "        )\n",
    "\n",
    "        conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer2.conv1d_in_channel,\n",
    "            cfg.model.stn.layer2.conv1d_out_channel,\n",
    "            cfg.model.stn.layer2.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer3.conv1d_in_channel,\n",
    "            cfg.model.stn.layer3.conv1d_out_channel,\n",
    "            cfg.model.stn.layer3.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        fc1 = nn.Linear(cfg.model.stn.layer4.fc1_in_features, cfg.model.stn.layer4.fc1_out_features)\n",
    "        fc2 = nn.Linear(cfg.model.stn.layer5.fc2_in_features, cfg.model.stn.layer5.fc2_out_features)\n",
    "        fc3 = nn.Linear(cfg.model.stn.layer6.fc3_in_features, self.k*self.k)\n",
    "    \n",
    "        bn1 = nn.BatchNorm1d(cfg.model.stn.layer1.conv1d_out_channel)\n",
    "        bn2 = nn.BatchNorm1d(cfg.model.stn.layer2.conv1d_out_channel)\n",
    "        bn3 = nn.BatchNorm1d(cfg.model.stn.layer3.conv1d_out_channel)\n",
    "        bn4 = nn.BatchNorm1d(cfg.model.stn.layer4.fc1_out_features)\n",
    "        bn5 = nn.BatchNorm1d(cfg.model.stn.layer5.fc3_out_features)\n",
    "        \n",
    "        relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "39a5ebaf-7de9-420d-8549-cf31f4814375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt:\n",
      "  optimizers:\n",
      "  - name: Adam\n",
      "    kwargs:\n",
      "      lr: 0.001\n",
      "  lr_schedulers:\n",
      "  - name: LinearWarmupLR\n",
      "    kwargs:\n",
      "      warmup_end_steps: 1000\n",
      "data:\n",
      "  name: shapenet\n",
      "  data_root: /root/share/pointnet.pytorch/pointnet/data\n",
      "  num_points: 2500\n",
      "model:\n",
      "  name: PointNet\n",
      "  cls:\n",
      "    layer1:\n",
      "      in_feature: 1024\n",
      "      out_feature: 512\n",
      "    layer2:\n",
      "      in_feature: 512\n",
      "      out_feature: 256\n",
      "    layer3:\n",
      "      in_feature: 256\n",
      "      out_feature: 26\n",
      "    dropout: 0.3\n",
      "  densecls:\n",
      "    k: 26\n",
      "    layer1:\n",
      "      conv1d_in_channel: 1088\n",
      "      conv1d_out_channel: 512\n",
      "      conv1d_kernel_size: 1\n",
      "    layer2:\n",
      "      conv1d_in_channel: 512\n",
      "      conv1d_out_channel: 256\n",
      "      conv1d_kernel_size: 1\n",
      "    layer3:\n",
      "      conv1d_in_channel: 256\n",
      "      conv1d_out_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "    layer4:\n",
      "      conv1d_in_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "  feat:\n",
      "    layer1:\n",
      "      conv1d_in_channel: 3\n",
      "      conv1d_out_channel: 64\n",
      "      conv1d_kernel_size: 1\n",
      "    layer2:\n",
      "      conv1d_in_channel: 64\n",
      "      conv1d_out_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "    layer3:\n",
      "      conv1d_in_channel: 128\n",
      "      conv1d_out_channel: 1024\n",
      "      conv1d_kernel_size: 1\n",
      "    global_feat: true\n",
      "    feature_transform: true\n",
      "  stn:\n",
      "    k: 64\n",
      "    layer1:\n",
      "      conv1d_in_channel: 3\n",
      "      conv1d_out_channel: 64\n",
      "      conv1d_kernel_size: 1\n",
      "    layer2:\n",
      "      conv1d_in_channel: 64\n",
      "      conv1d_out_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "    layer3:\n",
      "      conv1d_in_channel: 128\n",
      "      conv1d_out_channel: 1024\n",
      "      conv1d_kernel_size: 1\n",
      "    layer4:\n",
      "      fc2_in_features: 1024\n",
      "      fc2_out_features: 512\n",
      "    layer5:\n",
      "      fc2_in_features: 512\n",
      "      fc2_out_features: 256\n",
      "    layer6:\n",
      "      fc3_in_features: 256\n",
      "      fc3_out_features: 9\n",
      "train:\n",
      "  train_batch_size: 32\n",
      "  val_batch_size: 16\n",
      "  test_batch_size: 32\n",
      "  train_val_split:\n",
      "  - 0.9\n",
      "  - 0.1\n",
      "  run_root_dir: /root/share/pointnet.pytorch/pointnet/runs/pointnet-runs/2024-06-04T07:14:58-PointNet-shapenet\n",
      "  trainer_kwargs:\n",
      "    accelerator: auto\n",
      "    max_epochs: 50\n",
      "    val_check_interval: 1.0\n",
      "    log_every_n_steps: 100\n",
      "log:\n",
      "  loggers:\n",
      "    WandbLogger:\n",
      "      project: pointNet\n",
      "      name: 2024-06-04T07:14:58-PointNet-shapenet\n",
      "      tags:\n",
      "      - fastcampus_de_en_translate_tutorials\n",
      "      save_dir: /root/share/pointnet.pytorch/pointnet/runs/pointnet-runs/2024-06-04T07:14:58-PointNet-shapenet\n",
      "  callbacks:\n",
      "    ModelCheckpoint:\n",
      "      save_top_k: 3\n",
      "      monitor: val_loss\n",
      "      mode: min\n",
      "      verbose: true\n",
      "      dirpath: /root/share/pointnet.pytorch/pointnet/runs/pointnet-runs/2024-06-04T07:14:58-PointNet-shapenet/weights\n",
      "      filename: '{epoch}-{val_loss:.3f}'\n",
      "    EarlyStopping:\n",
      "      monitor: val_loss\n",
      "      mode: min\n",
      "      patience: 3\n",
      "      verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93054/4203120565.py:137: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=None)\n"
     ]
    }
   ],
   "source": [
    "# model configs\n",
    "model_pointnet_cfg = {\n",
    "    \"name\": \"PointNet\",\n",
    "    \"cls\": {\n",
    "        \"layer1\":{\n",
    "            \"in_feature\": 1024,\n",
    "            \"out_feature\": 512,\n",
    "        },\n",
    "        \"layer2\":{\n",
    "            \"in_feature\": 512,\n",
    "            \"out_feature\": 256,\n",
    "        },\n",
    "        \"layer3\":{\n",
    "            \"in_feature\": 256,\n",
    "            \"out_feature\": 26, ################## \n",
    "        },\n",
    "        \"dropout\" : 0.3,\n",
    "\n",
    "    },\n",
    "    \"densecls\": {\n",
    "        \"k\" : 26, ##################\n",
    "        \"layer1\": {\n",
    "            \"conv1d_in_channel\": 1088,\n",
    "            \"conv1d_out_channel\": 512,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "            \"conv1d_in_channel\": 512,\n",
    "            \"conv1d_out_channel\": 256,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer3\": {\n",
    "            \"conv1d_in_channel\": 256,\n",
    "            \"conv1d_out_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer4\":{\n",
    "            \"conv1d_in_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "    },\n",
    "    \"feat\":{\n",
    "        \"layer1\": {\n",
    "            \"conv1d_in_channel\": 3,\n",
    "            \"conv1d_out_channel\": 64,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "         \"layer2\": {\n",
    "            \"conv1d_in_channel\": 64,\n",
    "            \"conv1d_out_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "         \"layer3\": {\n",
    "            \"conv1d_in_channel\": 128,\n",
    "            \"conv1d_out_channel\": 1024,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"global_feat\": True,\n",
    "        \"feature_transform\": True,\n",
    "    },\n",
    "    \"stn\": {\n",
    "        \"k\": 64, \n",
    "        \"layer1\": {\n",
    "          \"conv1d_in_channel\": 3,\n",
    "          \"conv1d_out_channel\": 64,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "          \"conv1d_in_channel\": 64,\n",
    "          \"conv1d_out_channel\": 128,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer3\": {\n",
    "          \"conv1d_in_channel\": 128,\n",
    "          \"conv1d_out_channel\": 1024,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer4\": {\n",
    "          \"fc2_in_features\": 1024,\n",
    "          \"fc2_out_features\": 512,\n",
    "        },\n",
    "        \"layer5\": {\n",
    "          \"fc2_in_features\": 512,\n",
    "          \"fc2_out_features\": 256,\n",
    "        },\n",
    "        \"layer6\": {\n",
    "          \"fc3_in_features\": 256,\n",
    "          \"fc3_out_features\": 9,\n",
    "        }  \n",
    "    }\n",
    "}\n",
    "\n",
    "        \n",
    "# optimizer configs\n",
    "opt_cfg = {\n",
    "    \"optimizers\": [\n",
    "        {\n",
    "            \"name\": \"Adam\",\n",
    "            \"kwargs\": {\n",
    "                \"lr\": 1e-3,\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"lr_schedulers\": [\n",
    "        {\n",
    "            \"name\": \"LinearWarmupLR\",\n",
    "            \"kwargs\": {\n",
    "                \"warmup_end_steps\": 1000\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_shapenet_cfg = {\n",
    "    \"name\" : \"shapenet\",\n",
    "    \"data_root\" : os.path.join(os.getcwd(), \"data\"),\n",
    "    \"num_points\" : 2500\n",
    "}\n",
    "\n",
    "_merged_cfg_presets = {\n",
    "    \"PointNet\": {\n",
    "        \"opt\": opt_cfg,\n",
    "        \"data\": data_shapenet_cfg,\n",
    "        \"model\": model_pointnet_cfg,\n",
    "    },\n",
    "}\n",
    "\n",
    "# clear config instance first\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "# register preset configs\n",
    "register_config(_merged_cfg_presets)\n",
    "\n",
    "# initialize & make config\n",
    "## select mode here ##\n",
    "# .................. #\n",
    "hydra.initialize(config_path=None)\n",
    "cfg = hydra.compose(\"PointNet\")\n",
    "\n",
    "# override some cfg\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n",
    "\n",
    "# Define other train configs & log_configs\n",
    "# Merge configs into one & register it to Hydra.\n",
    "\n",
    "project_root_dir = os.path.join(\n",
    "    os.getcwd(), \"runs\", \"pointnet-runs\"\n",
    ")\n",
    "save_dir = os.path.join(project_root_dir, run_name)\n",
    "run_root_dir = os.path.join(project_root_dir, run_name)\n",
    "\n",
    "train_cfg = {\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"train_val_split\": [0.9, 0.1],\n",
    "    \"run_root_dir\": run_root_dir,\n",
    "    \"trainer_kwargs\": {\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"max_epochs\": 50,\n",
    "        \"val_check_interval\": 1.0,\n",
    "        \"log_every_n_steps\": 100,\n",
    "    }\n",
    "}\n",
    "\n",
    "# logger config\n",
    "log_cfg = {\n",
    "    \"loggers\": {\n",
    "        \"WandbLogger\": {\n",
    "            \"project\": \"pointNet\",\n",
    "            \"name\": run_name,\n",
    "            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n",
    "            \"save_dir\": run_root_dir,\n",
    "        },\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"ModelCheckpoint\": {\n",
    "            \"save_top_k\": 3,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"verbose\": True,\n",
    "            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n",
    "            \"filename\": \"{epoch}-{val_loss:.3f}\",\n",
    "        },\n",
    "        \"EarlyStopping\": {\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"patience\": 3,\n",
    "            \"verbose\": True,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# unlock config & set train_cfg & log_cfg\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "cfg.train = train_cfg\n",
    "cfg.log = log_cfg\n",
    "\n",
    "# lock config\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273266e2-e7a4-4eeb-babf-d5fac17387b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = cfg.data.data_root\n",
    "\n",
    "# 천처리 부분 (preprocessing) & 데이터 셋 정의.\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            cfg.model.feature.normalize.mean,\n",
    "            cfg.model.feature.normalize.std,\n",
    "        ) # mean, # std\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ShapeNetDataset(\n",
    "    root=data_root, \n",
    "    classification=True, \n",
    "    npoints=cfg.data.num_points\n",
    ")\n",
    "\n",
    "test_dataset = ShapeNetDataset(\n",
    "    root=data_root,\n",
    "    classification=True,\n",
    "    split='test',\n",
    "    npoints=cfg.data.num_point,\n",
    "    data_augmentation=False\n",
    ")\n",
    "        \n",
    "\n",
    "datasets = dataset_split(dataset, split=cfg.train.train_val_split)\n",
    "\n",
    "train_dataset = datasets[\"train\"]\n",
    "val_dataset = datasets[\"val\"]\n",
    "\n",
    "train_batch_size = cfg.train.train_batch_size\n",
    "val_batch_size = cfg.train.val_batch_size\n",
    "test_batch_size = cfg.train.test_batch_size\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
