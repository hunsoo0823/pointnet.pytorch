{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe6c3ff-3fd1-4daf-ab11-04b3bc373862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "from typing import Optional\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from omegaconf import OmegaConf\n",
    "from omegaconf import DictConfig\n",
    "import hydra\n",
    "from hydra.core.config_store import ConfigStore\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bbcb1ee-2541-4f7c-8680-27eabe8629d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/root/pointnet.pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4a799d-433d-44a0-92ac-a30cf8df9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import dataset_split\n",
    "from utils.config_utils import flatten_dict\n",
    "from utils.config_utils import register_config\n",
    "from utils.config_utils import configure_optimizers_from_cfg\n",
    "from utils.config_utils import get_loggers\n",
    "from utils.config_utils import get_callbacks\n",
    "from utils.custom_math import softmax\n",
    "from pointnet.dataset import ShapeNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0ef6695-6340-44e9-a1aa-6bdbb4452dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseLightningModule(pl.LightningModule):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # self.loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, inputs, target):\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        self._optimizer, self._schedulers = configure_optimizers_from_cfg(self.cfg, self)\n",
    "        return self._optimizer, self._schedulers\n",
    "\n",
    "    def _forward(self, points, target, mode: str):\n",
    "\n",
    "        assert mode in [\"train\", \"val\", \"test\"]\n",
    "\n",
    "        current_loss = 0.0\n",
    "        current_corrects = 0\n",
    "\n",
    "        # get predictions\n",
    "        #outputs = self(images)\n",
    "        target = target[:, 0]\n",
    "        points = points.transpose(2, 1)\n",
    "\n",
    "        pred, trans, trans_feat = model(points)\n",
    "\n",
    "        # get loss(Loss 계산)\n",
    "        loss = F.nll_loss(pred, target)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "        corrects = torch.sum(pred_choice == target.data)\n",
    "        acc = corrects / len(pred)\n",
    "\n",
    "        return {\n",
    "            f\"{mode}_loss\": loss,\n",
    "            f\"{mode}_acc\": acc,\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        points, target = batch\n",
    "        logs = self._forward(points, target, mode=\"train\")\n",
    "        self.log_dict(logs)\n",
    "        logs[\"loss\"] = logs[\"train_loss\"]\n",
    "        cul_lr = self._optimizer.param_groups[0][\"lr\"] if self._schedulers is None else self._schedulers[0].get_last_lr()[0]\n",
    "        wandb.log({\"learning_rate\": cul_lr})\n",
    "\n",
    "        return logs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        points, target = batch\n",
    "        logs = self._forward(points, target, mode=\"val\")\n",
    "        self.log_dict(logs)\n",
    "        logs[\"loss\"] = logs[\"val_loss\"]\n",
    "        return logs\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        points, target = batch\n",
    "        logs = self._forward(points, target, mode=\"test\")\n",
    "        self.log_dict(logs)\n",
    "        logs[\"loss\"] = logs[\"test_loss\"]\n",
    "        return logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc08bc7-57b7-4ec9-b695-31d26a708413",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetCls(BaseLightningModule):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        BaseLightningModule.__init__(self, cfg=cfg)\n",
    "        \n",
    "        self.feat = PointNetfeat(cfg)\n",
    "        self.fc1 = nn.Linear(cfg.model.cls.layer1.in_feature, cfg.model.cls.layer1.out_feature)\n",
    "        self.fc2 = nn.Linear(cfg.model.cls.layer2.in_feature, cfg.model.cls.layer2.out_feature)\n",
    "        self.fc3 = nn.Linear(cfg.model.cls.layer3.in_feature, cfg.model.cls.layer3.out_feature)\n",
    "        self.dropout = nn.Dropout(cfg.model.cls.dropout)\n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.cls.layer1.out_feature)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.cls.layer2.out_feature)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1), trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9c26e6-d7de-4fbc-b158-ba760fda1ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetDenseCls(BaseLightningModule):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        BaseLightningModule.__init__(self, cfg=cfg)\n",
    "        \n",
    "        self.k = cfg.model.densecls.k\n",
    "        self.feat = PointNetfeat(cfg)\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer1.conv1d_in_channel,\n",
    "            cfg.model.densecls.layer1.conv1d_out_channel,\n",
    "            cfg.model.densecls.layer1.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer2.conv1d_in_channel,\n",
    "            cfg.model.densecls.layer2.conv1d_out_channel,\n",
    "            cfg.model.densecls.layer2.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer3.conv1d_in_channel,\n",
    "            cfg.model.densecls.layer3.conv1d_out_channel,\n",
    "            cfg.model.densecls.layer3.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv4 = torch.nn.Conv1d(\n",
    "            cfg.model.densecls.layer4.conv1d_in_channel,\n",
    "            self.k,\n",
    "            cfg.model.densecls.layer3.conv1d_kernel_size\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.densecls.layer1.conv1d_out_channel)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.densecls.layer2.conv1d_out_channel)\n",
    "        self.bn3 = nn.BatchNorm1d(cfg.model.densecls.layer3.conv1d_out_channel)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
    "        x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b09a6d9e-6709-4a33-96a2-89ae15a0a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.stn = STN3d(cfg)\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            cfg.model.feat.layer1.conv1d_in_channel,\n",
    "            cfg.model.feat.layer1.conv1d_out_channel,\n",
    "            cfg.model.feat.layer1.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.feat.layer2.conv1d_in_channel,\n",
    "            cfg.model.feat.layer2.conv1d_out_channel,\n",
    "            cfg.model.feat.layer2.conv1d_kernel_size\n",
    "        )\n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.feat.layer3.conv1d_in_channel,\n",
    "            cfg.model.feat.layer3.conv1d_out_channel,\n",
    "            cfg.model.feat.layer3.conv1d_kernel_size\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.feat.layer1.conv1d_out_channel)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.feat.layer2.conv1d_out_channel)\n",
    "        self.bn3 = nn.BatchNorm1d(cfg.model.feat.layer3.conv1d_out_channel)\n",
    "        self.global_feat = cfg.model.feat.global_feat\n",
    "        self.feature_transform = cfg.model.feat.feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(cfg)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7afa8dbd-b64d-47cf-9954-c7de036bb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STN3d(nn.Module):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer1.conv1d_in_channel,\n",
    "            cfg.model.stn.layer1.conv1d_out_channel,\n",
    "            cfg.model.stn.layer1.conv1d_kernel_size\n",
    "        )\n",
    "\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer2.conv1d_in_channel,\n",
    "            cfg.model.stn.layer2.conv1d_out_channel,\n",
    "            cfg.model.stn.layer2.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer3.conv1d_in_channel,\n",
    "            cfg.model.stn.layer3.conv1d_out_channel,\n",
    "            cfg.model.stn.layer3.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        self.fc1 = nn.Linear(cfg.model.stn.layer4.fc1_in_features, cfg.model.stn.layer4.fc1_out_features)\n",
    "        self.fc2 = nn.Linear(cfg.model.stn.layer5.fc2_in_features, cfg.model.stn.layer5.fc2_out_features)\n",
    "        self.fc3 = nn.Linear(cfg.model.stn.layer6.fc3_in_features, cfg.model.stn.layer6.fc3_out_features)\n",
    "    \n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.stn.layer1.conv1d_out_channel)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.stn.layer2.conv1d_out_channel)\n",
    "        self.bn3 = nn.BatchNorm1d(cfg.model.stn.layer3.conv1d_out_channel)\n",
    "        self.bn4 = nn.BatchNorm1d(cfg.model.stn.layer4.fc1_out_features)\n",
    "        self.bn5 = nn.BatchNorm1d(cfg.model.stn.layer5.fc2_out_features)\n",
    "        \n",
    "        relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,.0,0,1]).astype(np.float32)))view(1,9).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, 3, 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57c320e8-49cb-4da5-aaec-f60535b4647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STNkd(nn.Module):\n",
    "    def __init__(self, cfg: DictConfig):\n",
    "        super().__init__()\n",
    "        self.k = cfg.model.stn.k\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv1d(\n",
    "            self.k,\n",
    "            cfg.model.stn.layer1.conv1d_out_channel,\n",
    "            cfg.model.stn.layer1.conv1d_kernel_size\n",
    "        )\n",
    "\n",
    "        self.conv2 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer2.conv1d_in_channel,\n",
    "            cfg.model.stn.layer2.conv1d_out_channel,\n",
    "            cfg.model.stn.layer2.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        self.conv3 = torch.nn.Conv1d(\n",
    "            cfg.model.stn.layer3.conv1d_in_channel,\n",
    "            cfg.model.stn.layer3.conv1d_out_channel,\n",
    "            cfg.model.stn.layer3.conv1d_kernel_size\n",
    "        )\n",
    "    \n",
    "        self.fc1 = nn.Linear(cfg.model.stn.layer4.fc1_in_features, cfg.model.stn.layer4.fc1_out_features)\n",
    "        self.fc2 = nn.Linear(cfg.model.stn.layer5.fc2_in_features, cfg.model.stn.layer5.fc2_out_features)\n",
    "        self.fc3 = nn.Linear(cfg.model.stn.layer6.fc3_in_features, self.k*self.k)\n",
    "    \n",
    "        self.bn1 = nn.BatchNorm1d(cfg.model.stn.layer1.conv1d_out_channel)\n",
    "        self.bn2 = nn.BatchNorm1d(cfg.model.stn.layer2.conv1d_out_channel)\n",
    "        self.bn3 = nn.BatchNorm1d(cfg.model.stn.layer3.conv1d_out_channel)\n",
    "        self.bn4 = nn.BatchNorm1d(cfg.model.stn.layer4.fc1_out_features)\n",
    "        self.bn5 = nn.BatchNorm1d(cfg.model.stn.layer5.fc2_out_features)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c37ecd-82f1-479c-b383-07093e46a648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt:\n",
      "  optimizers:\n",
      "  - name: RAdam\n",
      "    kwargs:\n",
      "      lr: 0.001\n",
      "  lr_schedulers:\n",
      "  - name: LinearWarmupLR\n",
      "    kwargs:\n",
      "      warm_end_steps: 1000\n",
      "data:\n",
      "  name: shapenet\n",
      "  data_root: /root/pointnet.pytorch/data\n",
      "  num_points: 2500\n",
      "model:\n",
      "  name: PointNet\n",
      "  cls:\n",
      "    layer1:\n",
      "      in_feature: 1024\n",
      "      out_feature: 512\n",
      "    layer2:\n",
      "      in_feature: 512\n",
      "      out_feature: 256\n",
      "    layer3:\n",
      "      in_feature: 256\n",
      "      out_feature: 16\n",
      "    dropout: 0.3\n",
      "  densecls:\n",
      "    k: 16\n",
      "    layer1:\n",
      "      conv1d_in_channel: 1088\n",
      "      conv1d_out_channel: 512\n",
      "      conv1d_kernel_size: 1\n",
      "    layer2:\n",
      "      conv1d_in_channel: 512\n",
      "      conv1d_out_channel: 256\n",
      "      conv1d_kernel_size: 1\n",
      "    layer3:\n",
      "      conv1d_in_channel: 256\n",
      "      conv1d_out_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "    layer4:\n",
      "      conv1d_in_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "  feat:\n",
      "    layer1:\n",
      "      conv1d_in_channel: 3\n",
      "      conv1d_out_channel: 64\n",
      "      conv1d_kernel_size: 1\n",
      "    layer2:\n",
      "      conv1d_in_channel: 64\n",
      "      conv1d_out_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "    layer3:\n",
      "      conv1d_in_channel: 128\n",
      "      conv1d_out_channel: 1024\n",
      "      conv1d_kernel_size: 1\n",
      "    global_feat: true\n",
      "    feature_transform: true\n",
      "  stn:\n",
      "    k: 64\n",
      "    layer1:\n",
      "      conv1d_in_channel: 3\n",
      "      conv1d_out_channel: 64\n",
      "      conv1d_kernel_size: 1\n",
      "    layer2:\n",
      "      conv1d_in_channel: 64\n",
      "      conv1d_out_channel: 128\n",
      "      conv1d_kernel_size: 1\n",
      "    layer3:\n",
      "      conv1d_in_channel: 128\n",
      "      conv1d_out_channel: 1024\n",
      "      conv1d_kernel_size: 1\n",
      "    layer4:\n",
      "      fc1_in_features: 1024\n",
      "      fc1_out_features: 512\n",
      "    layer5:\n",
      "      fc2_in_features: 512\n",
      "      fc2_out_features: 256\n",
      "    layer6:\n",
      "      fc3_in_features: 256\n",
      "      fc3_out_features: 9\n",
      "train:\n",
      "  train_batch_size: 32\n",
      "  val_batch_size: 16\n",
      "  test_batch_size: 32\n",
      "  train_val_split:\n",
      "  - 0.9\n",
      "  - 0.1\n",
      "  run_root_dir: /root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet\n",
      "  trainer_kwargs:\n",
      "    accelerator: auto\n",
      "    max_epochs: 50\n",
      "    val_check_interval: 1.0\n",
      "    log_every_n_steps: 100\n",
      "log:\n",
      "  loggers:\n",
      "    WandbLogger:\n",
      "      project: pointNet\n",
      "      name: 2025-01-13T16:39:57-PointNet-shapenet\n",
      "      tags:\n",
      "      - fastcampus_de_en_translate_tutorials\n",
      "      save_dir: /root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet\n",
      "  callbacks:\n",
      "    ModelCheckpoint:\n",
      "      save_top_k: 3\n",
      "      monitor: val_loss\n",
      "      mode: min\n",
      "      verbose: true\n",
      "      dirpath: /root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights\n",
      "      filename: '{epoch}-{val_loss:.3f}'\n",
      "    EarlyStopping:\n",
      "      monitor: val_loss\n",
      "      mode: min\n",
      "      patience: 3\n",
      "      verbose: true\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87119/211539327.py:137: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  hydra.initialize(config_path=None)\n"
     ]
    }
   ],
   "source": [
    "# model configs\n",
    "model_pointnet_cfg = {\n",
    "    \"name\": \"PointNet\",\n",
    "    \"cls\": {\n",
    "        \"layer1\":{\n",
    "            \"in_feature\": 1024,\n",
    "            \"out_feature\": 512,\n",
    "        },\n",
    "        \"layer2\":{\n",
    "            \"in_feature\": 512,\n",
    "            \"out_feature\": 256,\n",
    "        },\n",
    "        \"layer3\":{\n",
    "            \"in_feature\": 256,\n",
    "            \"out_feature\": 16, \n",
    "        },\n",
    "        \"dropout\" : 0.3,\n",
    "\n",
    "    },\n",
    "    \"densecls\": {\n",
    "        \"k\" : 16,  ########\n",
    "        \"layer1\": {\n",
    "            \"conv1d_in_channel\": 1088,\n",
    "            \"conv1d_out_channel\": 512,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "            \"conv1d_in_channel\": 512,\n",
    "            \"conv1d_out_channel\": 256,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer3\": {\n",
    "            \"conv1d_in_channel\": 256,\n",
    "            \"conv1d_out_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer4\":{\n",
    "            \"conv1d_in_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "    },\n",
    "    \"feat\":{\n",
    "        \"layer1\": {\n",
    "            \"conv1d_in_channel\": 3,\n",
    "            \"conv1d_out_channel\": 64,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "         \"layer2\": {\n",
    "            \"conv1d_in_channel\": 64,\n",
    "            \"conv1d_out_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "         \"layer3\": {\n",
    "            \"conv1d_in_channel\": 128,\n",
    "            \"conv1d_out_channel\": 1024,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"global_feat\": True,\n",
    "        \"feature_transform\": True,\n",
    "    },\n",
    "    \"stn\": {\n",
    "        \"k\": 64, \n",
    "        \"layer1\": {\n",
    "          \"conv1d_in_channel\": 3,\n",
    "          \"conv1d_out_channel\": 64,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "          \"conv1d_in_channel\": 64,\n",
    "          \"conv1d_out_channel\": 128,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer3\": {\n",
    "          \"conv1d_in_channel\": 128,\n",
    "          \"conv1d_out_channel\": 1024,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer4\": {\n",
    "          \"fc1_in_features\": 1024,\n",
    "          \"fc1_out_features\": 512,\n",
    "        },\n",
    "        \"layer5\": {\n",
    "          \"fc2_in_features\": 512,\n",
    "          \"fc2_out_features\": 256,\n",
    "        },\n",
    "        \"layer6\": {\n",
    "          \"fc3_in_features\": 256,\n",
    "          \"fc3_out_features\": 9,\n",
    "        }  \n",
    "    }\n",
    "}\n",
    "\n",
    "        \n",
    "# optimizer configs\n",
    "opt_cfg = {\n",
    "    \"optimizers\": [\n",
    "        {\n",
    "            \"name\": \"RAdam\",\n",
    "            \"kwargs\": {\n",
    "                \"lr\": 1e-3,\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"lr_schedulers\": [\n",
    "        {\n",
    "            \"name\": \"LinearWarmupLR\",\n",
    "            \"kwargs\": {\n",
    "                \"warm_end_steps\": 1000\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_shapenet_cfg = {\n",
    "    \"name\" : \"shapenet\",\n",
    "    \"data_root\" : os.path.join(os.getcwd(), \"data\"),\n",
    "    \"num_points\" : 2500\n",
    "}\n",
    "\n",
    "_merged_cfg_presets = {\n",
    "    \"PointNet\": {\n",
    "        \"opt\": opt_cfg,\n",
    "        \"data\": data_shapenet_cfg,\n",
    "        \"model\": model_pointnet_cfg,\n",
    "    },\n",
    "}\n",
    "\n",
    "# clear config instance first\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "# register preset configs\n",
    "register_config(_merged_cfg_presets)\n",
    "\n",
    "# initialize & mae config\n",
    "## select mode here ##\n",
    "# .................. #\n",
    "hydra.initialize(config_path=None)\n",
    "cfg = hydra.compose(\"PointNet\")\n",
    "\n",
    "# override some cfg\n",
    "run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n",
    "\n",
    "# Define other train configs & log_configs\n",
    "# Merge configs into one & register it to Hydra.\n",
    "\n",
    "project_root_dir = os.path.join(\n",
    "    os.getcwd(), \"runs\", \"pointnet-runs\"\n",
    ")\n",
    "save_dir = os.path.join(project_root_dir, run_name)\n",
    "run_root_dir = os.path.join(project_root_dir, run_name)\n",
    "\n",
    "train_cfg = {\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"train_val_split\": [0.9, 0.1],\n",
    "    \"run_root_dir\": run_root_dir,\n",
    "    \"s\": {\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"max_epochs\": 50,\n",
    "        \"val_check_interval\": 1.0,\n",
    "        \"log_every_n_steps\": 100,\n",
    "    }\n",
    "}\n",
    "\n",
    "# logger config\n",
    "log_cfg = {\n",
    "    \"loggers\": {\n",
    "        \"WandbLogger\": {\n",
    "            \"project\": \"pointNet\",\n",
    "            \"name\": run_name,\n",
    "            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n",
    "            \"save_dir\": run_root_dir,\n",
    "        },\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"ModelCheckpoint\": {\n",
    "            \"save_top_k\": 3,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"verbose\": True,\n",
    "            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n",
    "            \"filename\": \"{epoch}-{val_loss:.3f}\",\n",
    "        },\n",
    "        \"EarlyStopping\": {\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"patience\": 3,\n",
    "            \"verbose\": True,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# unlock config & set train_cfg & log_cfg\n",
    "OmegaConf.set_struct(cfg, False)\n",
    "cfg.train = train_cfg\n",
    "cfg.log = log_cfg\n",
    "\n",
    "# lock config\n",
    "OmegaConf.set_struct(cfg, True)\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "273266e2-e7a4-4eeb-babf-d5fac17387b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15}\n",
      "{'Airplane': 4, 'Bag': 2, 'Cap': 2, 'Car': 4, 'Chair': 4, 'Earphone': 3, 'Guitar': 3, 'Knife': 2, 'Lamp': 4, 'Laptop': 2, 'Motorbike': 6, 'Mug': 2, 'Pistol': 3, 'Rocket': 3, 'Skateboard': 3, 'Table': 3} 4\n",
      "{'Airplane': 0, 'Bag': 1, 'Cap': 2, 'Car': 3, 'Chair': 4, 'Earphone': 5, 'Guitar': 6, 'Knife': 7, 'Lamp': 8, 'Laptop': 9, 'Motorbike': 10, 'Mug': 11, 'Pistol': 12, 'Rocket': 13, 'Skateboard': 14, 'Table': 15}\n",
      "{'Airplane': 4, 'Bag': 2, 'Cap': 2, 'Car': 4, 'Chair': 4, 'Earphone': 3, 'Guitar': 3, 'Knife': 2, 'Lamp': 4, 'Laptop': 2, 'Motorbike': 6, 'Mug': 2, 'Pistol': 3, 'Rocket': 3, 'Skateboard': 3, 'Table': 3} 4\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "data_root = cfg.data.data_root\n",
    "\n",
    "dataset = ShapeNetDataset(\n",
    "    root=data_root, \n",
    "    classification=True, \n",
    "    npoints=cfg.data.num_points\n",
    ")\n",
    "\n",
    "test_dataset = ShapeNetDataset(\n",
    "    root=data_root,\n",
    "    classification=True,\n",
    "    split='test',\n",
    "    npoints=cfg.data.num_points,\n",
    "    data_augmentation=False\n",
    ")\n",
    "\n",
    "num_classes = len(dataset.classes)\n",
    "print(num_classes)\n",
    "\n",
    "dataset = dataset_split(dataset, split=cfg.train.train_val_split)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"val\"]\n",
    "\n",
    "train_batch_size = cfg.train.train_batch_size\n",
    "val_batch_size = cfg.train.val_batch_size\n",
    "test_batch_size = cfg.train.test_batch_size\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=val_batch_size, shuffle=True, num_workers=0\n",
    ")\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e074b245-6f68-41e8-890f-246f3f59719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PointNetCls(\n",
      "  (feat): PointNetfeat(\n",
      "    (stn): STN3d(\n",
      "      (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fstn): STNkd(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# model define\n",
    "\n",
    "def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n",
    "\n",
    "    if cfg.model.name == \"PointNet\":\n",
    "        model = PointNetCls(cfg)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    if checkpoint_path is not None:\n",
    "        model = PLMLP.load_from_checkpoint(checkpoint_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_pl_model(cfg)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46b1fa7c-65cb-4326-8c45-e8c46b98b921",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "logger = get_loggers(cfg)\n",
    "callbacks = get_callbacks(cfg)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=cfg.train.run_root_dir,\n",
    "    num_sanity_val_steps=2,\n",
    "    **cfg.train.trainer_kwargs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d063bff-9335-430e-8093-bb2522cf3689",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/wandb/run-20250113_164430-0wuyf4yb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cni-tech/pointNet/runs/0wuyf4yb' target=\"_blank\">2025-01-13T16:39:57-PointNet-shapenet</a></strong> to <a href='https://wandb.ai/cni-tech/pointNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cni-tech/pointNet' target=\"_blank\">https://wandb.ai/cni-tech/pointNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cni-tech/pointNet/runs/0wuyf4yb' target=\"_blank\">https://wandb.ai/cni-tech/pointNet/runs/0wuyf4yb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type         | Params | Mode \n",
      "-------------------------------------------------\n",
      "0 | feat    | PointNetfeat | 2.8 M  | train\n",
      "1 | fc1     | Linear       | 524 K  | train\n",
      "2 | fc2     | Linear       | 131 K  | train\n",
      "3 | fc3     | Linear       | 4.1 K  | train\n",
      "4 | dropout | Dropout      | 0      | train\n",
      "5 | bn1     | BatchNorm1d  | 1.0 K  | train\n",
      "6 | bn2     | BatchNorm1d  | 512    | train\n",
      "7 | relu    | ReLU         | 0      | train\n",
      "-------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "13.861    Total estimated model params size (MB)\n",
      "39        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:476: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 342/342 [00:29<00:00, 11.57it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 2.704\n",
      "Epoch 0, global step 342: 'val_loss' reached 2.70415 (best 2.70415), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=0-val_loss=2.704.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 342/342 [00:28<00:00, 12.14it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.112 >= min_delta = 0.0. New best score: 2.592\n",
      "Epoch 1, global step 684: 'val_loss' reached 2.59188 (best 2.59188), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=1-val_loss=2.592.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 342/342 [00:28<00:00, 12.04it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.255 >= min_delta = 0.0. New best score: 2.337\n",
      "Epoch 2, global step 1026: 'val_loss' reached 2.33666 (best 2.33666), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=2-val_loss=2.337.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 342/342 [00:28<00:00, 12.08it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.548 >= min_delta = 0.0. New best score: 1.789\n",
      "Epoch 3, global step 1368: 'val_loss' reached 1.78894 (best 1.78894), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=3-val_loss=1.789.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 342/342 [00:28<00:00, 12.03it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.439 >= min_delta = 0.0. New best score: 1.350\n",
      "Epoch 4, global step 1710: 'val_loss' reached 1.34992 (best 1.34992), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=4-val_loss=1.350.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 342/342 [00:28<00:00, 12.09it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.499 >= min_delta = 0.0. New best score: 0.851\n",
      "Epoch 5, global step 2052: 'val_loss' reached 0.85081 (best 0.85081), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=5-val_loss=0.851.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 342/342 [00:28<00:00, 12.09it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.418 >= min_delta = 0.0. New best score: 0.433\n",
      "Epoch 6, global step 2394: 'val_loss' reached 0.43266 (best 0.43266), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=6-val_loss=0.433.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 342/342 [00:28<00:00, 12.10it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 2736: 'val_loss' reached 0.43988 (best 0.43266), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=7-val_loss=0.440.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 342/342 [00:28<00:00, 12.07it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.0. New best score: 0.406\n",
      "Epoch 8, global step 3078: 'val_loss' reached 0.40588 (best 0.40588), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=8-val_loss=0.406.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 342/342 [00:28<00:00, 12.07it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 3420: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 342/342 [00:28<00:00, 12.09it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.086 >= min_delta = 0.0. New best score: 0.320\n",
      "Epoch 10, global step 3762: 'val_loss' reached 0.32008 (best 0.32008), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=10-val_loss=0.320.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 342/342 [00:28<00:00, 12.08it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.096 >= min_delta = 0.0. New best score: 0.224\n",
      "Epoch 11, global step 4104: 'val_loss' reached 0.22400 (best 0.22400), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=11-val_loss=0.224.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 342/342 [00:28<00:00, 12.07it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 4446: 'val_loss' reached 0.28155 (best 0.22400), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=12-val_loss=0.282.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 342/342 [00:28<00:00, 12.08it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 4788: 'val_loss' reached 0.23202 (best 0.22400), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=13-val_loss=0.232.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 342/342 [00:28<00:00, 12.05it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.022 >= min_delta = 0.0. New best score: 0.202\n",
      "Epoch 14, global step 5130: 'val_loss' reached 0.20155 (best 0.20155), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=14-val_loss=0.202.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 342/342 [00:28<00:00, 12.05it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 5472: 'val_loss' reached 0.20227 (best 0.20155), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=15-val_loss=0.202.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 342/342 [00:28<00:00, 12.04it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.068 >= min_delta = 0.0. New best score: 0.134\n",
      "Epoch 16, global step 5814: 'val_loss' reached 0.13381 (best 0.13381), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=16-val_loss=0.134.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 342/342 [00:28<00:00, 12.04it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 6156: 'val_loss' reached 0.15671 (best 0.13381), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=17-val_loss=0.157.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 342/342 [00:28<00:00, 12.08it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.132\n",
      "Epoch 18, global step 6498: 'val_loss' reached 0.13202 (best 0.13202), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=18-val_loss=0.132.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 342/342 [00:28<00:00, 12.03it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 6840: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 342/342 [00:28<00:00, 12.09it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.128\n",
      "Epoch 20, global step 7182: 'val_loss' reached 0.12768 (best 0.12768), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=20-val_loss=0.128.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 342/342 [00:28<00:00, 12.06it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 7524: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 342/342 [00:28<00:00, 12.10it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.018 >= min_delta = 0.0. New best score: 0.110\n",
      "Epoch 22, global step 7866: 'val_loss' reached 0.11009 (best 0.11009), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=22-val_loss=0.110.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 342/342 [00:28<00:00, 12.04it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 8208: 'val_loss' reached 0.11696 (best 0.11009), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=23-val_loss=0.117.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 342/342 [00:28<00:00, 12.04it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.038 >= min_delta = 0.0. New best score: 0.072\n",
      "Epoch 24, global step 8550: 'val_loss' reached 0.07191 (best 0.07191), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=24-val_loss=0.072.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 342/342 [00:28<00:00, 12.08it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 8892: 'val_loss' was not in top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 342/342 [00:28<00:00, 12.04it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 9234: 'val_loss' reached 0.11057 (best 0.07191), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=26-val_loss=0.111.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 342/342 [00:28<00:00, 12.05it/s, v_num=f4yb]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 3 records. Best score: 0.072. Signaling Trainer to stop.\n",
      "Epoch 27, global step 9576: 'val_loss' reached 0.08747 (best 0.07191), saving model to '/root/pointnet.pytorch/runs/pointnet-runs/2025-01-13T16:39:57-PointNet-shapenet/weights/epoch=27-val_loss=0.087.ckpt' as top 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 342/342 [00:28<00:00, 12.01it/s, v_num=f4yb]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eff6149-eed0-48e6-9b6b-f3b9068caf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model configs\n",
    "model_pointnet_cfg = {\n",
    "    \"name\": \"PointNet\",\n",
    "    \"cls\": {\n",
    "        \"layer1\":{\n",
    "            \"in_feature\": 1024,\n",
    "            \"out_feature\": 512,\n",
    "        },\n",
    "        \"layer2\":{\n",
    "            \"in_feature\": 512,\n",
    "            \"out_feature\": 256,\n",
    "        },\n",
    "        \"layer3\":{\n",
    "            \"in_feature\": 256,\n",
    "            \"out_feature\": 16, \n",
    "        },\n",
    "        \"dropout\" : 0.3,\n",
    "\n",
    "    },\n",
    "    \"densecls\": {\n",
    "        \"k\" : 16,  ########\n",
    "        \"layer1\": {\n",
    "            \"conv1d_in_channel\": 1088,\n",
    "            \"conv1d_out_channel\": 512,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "            \"conv1d_in_channel\": 512,\n",
    "            \"conv1d_out_channel\": 256,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer3\": {\n",
    "            \"conv1d_in_channel\": 256,\n",
    "            \"conv1d_out_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer4\":{\n",
    "            \"conv1d_in_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "    },\n",
    "    \"feat\":{\n",
    "        \"layer1\": {\n",
    "            \"conv1d_in_channel\": 3,\n",
    "            \"conv1d_out_channel\": 64,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "         \"layer2\": {\n",
    "            \"conv1d_in_channel\": 64,\n",
    "            \"conv1d_out_channel\": 128,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "         \"layer3\": {\n",
    "            \"conv1d_in_channel\": 128,\n",
    "            \"conv1d_out_channel\": 1024,\n",
    "            \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"global_feat\": True,\n",
    "        \"feature_transform\": True,\n",
    "    },\n",
    "    \"stn\": {\n",
    "        \"k\": 64, \n",
    "        \"layer1\": {\n",
    "          \"conv1d_in_channel\": 3,\n",
    "          \"conv1d_out_channel\": 64,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer2\": {\n",
    "          \"conv1d_in_channel\": 64,\n",
    "          \"conv1d_out_channel\": 128,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer3\": {\n",
    "          \"conv1d_in_channel\": 128,\n",
    "          \"conv1d_out_channel\": 1024,\n",
    "          \"conv1d_kernel_size\": 1,\n",
    "        },\n",
    "        \"layer4\": {\n",
    "          \"fc1_in_features\": 1024,\n",
    "          \"fc1_out_features\": 512,\n",
    "        },\n",
    "        \"layer5\": {\n",
    "          \"fc2_in_features\": 512,\n",
    "          \"fc2_out_features\": 256,\n",
    "        },\n",
    "        \"layer6\": {\n",
    "          \"fc3_in_features\": 256,\n",
    "          \"fc3_out_features\": 9,\n",
    "        }  \n",
    "    }\n",
    "}\n",
    "\n",
    "        \n",
    "# optimizer configs\n",
    "opt_cfg = {\n",
    "    \"optimizers\": [\n",
    "        {\n",
    "            \"name\": \"RAdam\",\n",
    "            \"kwargs\": {\n",
    "                \"lr\": 1e-3,\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"lr_schedulers\": [\n",
    "        {\n",
    "            \"name\": \"LinearWarmupLR\",\n",
    "            \"kwargs\": {\n",
    "                \"warm_end_steps\": 1000\n",
    "            }\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "data_shapenet_cfg = {\n",
    "    \"name\" : \"shapenet\",\n",
    "    \"data_root\" : os.path.join(os.getcwd(), \"data\"),\n",
    "    \"num_points\" : 2500\n",
    "}\n",
    "\n",
    "_merged_cfg_presets = {\n",
    "    \"PointNet\": {\n",
    "        \"opt\": opt_cfg,\n",
    "        \"data\": data_shapenet_cfg,\n",
    "        \"model\": model_pointnet_cfg,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37f731d8-1297-49f8-93a6-9ca6c8b96446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "def save_yaml(cfg, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        yaml.dump(cfg, file, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# 저장할 파일 경로\n",
    "output_dir = \"./config_yaml\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 각각 YAML로 저장\n",
    "save_yaml(model_pointnet_cfg, os.path.join(output_dir, \"model_pointnet.yaml\"))\n",
    "save_yaml(opt_cfg, os.path.join(output_dir, \"optimizer.yaml\"))\n",
    "save_yaml(data_shapenet_cfg, os.path.join(output_dir, \"data_shapenet.yaml\"))\n",
    "save_yaml(_merged_cfg_presets, os.path.join(output_dir, \"merged_cfg_presets.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f51c9d4-cf72-4498-8243-2cfc04d40ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = {\n",
    "    \"train_batch_size\": 32,\n",
    "    \"val_batch_size\": 16,\n",
    "    \"test_batch_size\": 32,\n",
    "    \"train_val_split\": [0.9, 0.1],\n",
    "    \"run_root_dir\": run_root_dir,\n",
    "    \"trainer_kwargs\": {\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"max_epochs\": 50,\n",
    "        \"val_check_interval\": 1.0,\n",
    "        \"log_every_n_steps\": 100,\n",
    "    }\n",
    "}\n",
    "\n",
    "# logger config\n",
    "log_cfg = {\n",
    "    \"loggers\": {\n",
    "        \"WandbLogger\": {\n",
    "            \"project\": \"pointNet\",\n",
    "            \"name\": run_name,\n",
    "            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n",
    "            \"save_dir\": run_root_dir,\n",
    "        },\n",
    "    },\n",
    "    \"callbacks\": {\n",
    "        \"ModelCheckpoint\": {\n",
    "            \"save_top_k\": 3,\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"verbose\": True,\n",
    "            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n",
    "            \"filename\": \"{epoch}-{val_loss:.3f}\",\n",
    "        },\n",
    "        \"EarlyStopping\": {\n",
    "            \"monitor\": \"val_loss\",\n",
    "            \"mode\": \"min\",\n",
    "            \"patience\": 3,\n",
    "            \"verbose\": True,\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c22b7e3-1077-41c0-9f53-e16f0559c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "def save_yaml(cfg, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        yaml.dump(cfg, file, default_flow_style=False, sort_keys=False, allow_unicode=True)\n",
    "\n",
    "# 저장할 파일 경로\n",
    "output_dir = \"./config_yaml\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 각각 YAML로 저장\n",
    "save_yaml(train_cfg, os.path.join(output_dir, \"train.yaml\"))\n",
    "save_yaml(log_cfg, os.path.join(output_dir, \"log.yaml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e939c36-dfbc-4315-8816-186fea7b6363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93e97f6-d164-4f6e-92ce-225dfd5fb173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
